{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "code.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNgLJ2k91rvupVhtyCZ5tzg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JazJaz426/Honours/blob/main/code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qI8WOMEeBDFm",
        "outputId": "ecb88ef6-6790-4a4a-a422-dbe45def9565"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd '/content/drive/MyDrive/honours_code'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kn_S17wIBFX2",
        "outputId": "88bee8f6-35c6-4601-f175-e92150635902"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/honours_code\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Function"
      ],
      "metadata": {
        "id": "RfFUH0fHfN3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import torchvision.transforms as transforms\n",
        "from __future__ import print_function\n",
        "from PIL import Image\n",
        "import os\n",
        "import os.path\n",
        "import numpy as np\n",
        "import sys\n",
        "if sys.version_info[0] == 2:\n",
        "    import cPickle as pickle\n",
        "else:\n",
        "    import pickle\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import os\n",
        "import os.path\n",
        "import copy\n",
        "import hashlib\n",
        "import errno\n",
        "import numpy as np\n",
        "from numpy.testing import assert_array_almost_equal\n",
        "import torch\n",
        "import torch.nn.functional as F \n",
        "\n",
        "def check_integrity(fpath, md5):\n",
        "    if not os.path.isfile(fpath):\n",
        "        return False\n",
        "    md5o = hashlib.md5()\n",
        "    with open(fpath, 'rb') as f:\n",
        "        # read in 1MB chunks\n",
        "        for chunk in iter(lambda: f.read(1024 * 1024), b''):\n",
        "            md5o.update(chunk)\n",
        "    md5c = md5o.hexdigest()\n",
        "    if md5c != md5:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def download_url(url, root, filename, md5):\n",
        "    from six.moves import urllib\n",
        "\n",
        "    root = os.path.expanduser(root)\n",
        "    fpath = os.path.join(root, filename)\n",
        "\n",
        "    try:\n",
        "        os.makedirs(root)\n",
        "    except OSError as e:\n",
        "        if e.errno == errno.EEXIST:\n",
        "            pass\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "    # downloads file\n",
        "    if os.path.isfile(fpath) and check_integrity(fpath, md5):\n",
        "        print('Using downloaded and verified file: ' + fpath)\n",
        "    else:\n",
        "        try:\n",
        "            print('Downloading ' + url + ' to ' + fpath)\n",
        "            urllib.request.urlretrieve(url, fpath)\n",
        "        except:\n",
        "            if url[:5] == 'https':\n",
        "                url = url.replace('https:', 'http:')\n",
        "                print('Failed download. Trying https -> http instead.'\n",
        "                      ' Downloading ' + url + ' to ' + fpath)\n",
        "                urllib.request.urlretrieve(url, fpath)\n",
        "\n",
        "\n",
        "def list_dir(root, prefix=False):\n",
        "    \"\"\"List all directories at a given root\n",
        "\n",
        "    Args:\n",
        "        root (str): Path to directory whose folders need to be listed\n",
        "        prefix (bool, optional): If true, prepends the path to each result, otherwise\n",
        "            only returns the name of the directories found\n",
        "    \"\"\"\n",
        "    root = os.path.expanduser(root)\n",
        "    directories = list(\n",
        "        filter(\n",
        "            lambda p: os.path.isdir(os.path.join(root, p)),\n",
        "            os.listdir(root)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    if prefix is True:\n",
        "        directories = [os.path.join(root, d) for d in directories]\n",
        "\n",
        "    return directories\n",
        "\n",
        "\n",
        "def list_files(root, suffix, prefix=False):\n",
        "    \"\"\"List all files ending with a suffix at a given root\n",
        "\n",
        "    Args:\n",
        "        root (str): Path to directory whose folders need to be listed\n",
        "        suffix (str or tuple): Suffix of the files to match, e.g. '.png' or ('.jpg', '.png').\n",
        "            It uses the Python \"str.endswith\" method and is passed directly\n",
        "        prefix (bool, optional): If true, prepends the path to each result, otherwise\n",
        "            only returns the name of the files found\n",
        "    \"\"\"\n",
        "    root = os.path.expanduser(root)\n",
        "    files = list(\n",
        "        filter(\n",
        "            lambda p: os.path.isfile(os.path.join(root, p)) and p.endswith(suffix),\n",
        "            os.listdir(root)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    if prefix is True:\n",
        "        files = [os.path.join(root, d) for d in files]\n",
        "\n",
        "    return files\n",
        "\n",
        "# basic function\n",
        "def multiclass_noisify(y, P, random_state=0):\n",
        "    \"\"\" Flip classes according to transition probability matrix T.\n",
        "    It expects a number between 0 and the number of classes - 1.\n",
        "    \"\"\"\n",
        "    #print np.max(y), P.shape[0]\n",
        "    assert P.shape[0] == P.shape[1]\n",
        "    assert np.max(y) < P.shape[0]\n",
        "\n",
        "    # row stochastic matrix\n",
        "    assert_array_almost_equal(P.sum(axis=1), np.ones(P.shape[1]))\n",
        "    assert (P >= 0.0).all()\n",
        "\n",
        "    m = y.shape[0]\n",
        "    #print m\n",
        "    new_y = y.copy()\n",
        "    flipper = np.random.RandomState(random_state)\n",
        "    print(f'flip with random seed {random_state}')\n",
        "\n",
        "    for idx in np.arange(m):\n",
        "        i = y[idx]\n",
        "        # draw a vector with only an 1\n",
        "        flipped = flipper.multinomial(1, P[i, :], 1)[0]\n",
        "        new_y[idx] = np.where(flipped == 1)[0]\n",
        "\n",
        "    return new_y\n",
        "\n",
        "\n",
        "# noisify_pairflip call the function \"multiclass_noisify\"\n",
        "def noisify_pairflip(y_train, noise, random_state=None, nb_classes=10):\n",
        "    \"\"\"mistakes:\n",
        "        flip in the pair\n",
        "    \"\"\"\n",
        "    P = np.eye(nb_classes)\n",
        "    n = noise\n",
        "\n",
        "    if n > 0.0:\n",
        "        # 0 -> 1\n",
        "        P[0, 0], P[0, 1] = 1. - n, n\n",
        "        for i in range(1, nb_classes-1):\n",
        "            P[i, i], P[i, i + 1] = 1. - n, n\n",
        "        P[nb_classes-1, nb_classes-1], P[nb_classes-1, 0] = 1. - n, n\n",
        "\n",
        "        y_train_noisy = multiclass_noisify(y_train, P=P,\n",
        "                                           random_state=random_state)\n",
        "        actual_noise = (y_train_noisy != y_train).mean()\n",
        "        assert actual_noise > 0.0\n",
        "        print('Actual noise %.2f' % actual_noise)\n",
        "        y_train = y_train_noisy\n",
        "    #print P\n",
        "\n",
        "    return y_train, actual_noise\n",
        "\n",
        "def noisify_multiclass_symmetric(y_train, noise, random_state=None, nb_classes=10):\n",
        "    \"\"\"mistakes:\n",
        "        flip in the symmetric way\n",
        "    \"\"\"\n",
        "    P = np.ones((nb_classes, nb_classes))\n",
        "    n = noise\n",
        "    P = (n / (nb_classes - 1)) * P\n",
        "\n",
        "    if n > 0.0:\n",
        "        # 0 -> 1\n",
        "        P[0, 0] = 1. - n\n",
        "        for i in range(1, nb_classes-1):\n",
        "            P[i, i] = 1. - n\n",
        "        P[nb_classes-1, nb_classes-1] = 1. - n\n",
        "\n",
        "        y_train_noisy = multiclass_noisify(y_train, P=P,\n",
        "                                           random_state=random_state)\n",
        "        actual_noise = (y_train_noisy != y_train).mean()\n",
        "        assert actual_noise > 0.0\n",
        "        print('Actual noise %.2f' % actual_noise)\n",
        "        y_train = y_train_noisy\n",
        "    #print P\n",
        "\n",
        "    return y_train, actual_noise\n",
        "\n",
        "def noisify(dataset='mnist', nb_classes=10, train_labels=None, noise_type=None, noise_rate=0, random_state=0):\n",
        "    if noise_type == 'pairflip':\n",
        "        train_noisy_labels, actual_noise_rate = noisify_pairflip(train_labels, noise_rate, random_state=0, nb_classes=nb_classes)\n",
        "    if noise_type == 'symmetric':\n",
        "        train_noisy_labels, actual_noise_rate = noisify_multiclass_symmetric(train_labels, noise_rate, random_state=0, nb_classes=nb_classes)\n",
        "    return train_noisy_labels, actual_noise_rate\n",
        "\n",
        "def noisify_instance(train_data, train_labels, noise_rate):\n",
        "    if max(train_labels)>10:\n",
        "        num_class = 100\n",
        "    else:\n",
        "        num_class = 10\n",
        "    np.random.seed(0)\n",
        "\n",
        "    q_ = np.random.normal(loc=noise_rate,scale=0.1,size=1000000)\n",
        "    q = []\n",
        "    for pro in q_:\n",
        "        if 0 < pro < 1:\n",
        "            q.append(pro)\n",
        "        if len(q)==50000:\n",
        "            break\n",
        "\n",
        "    w = np.random.normal(loc=0,scale=1,size=(32*32*3,num_class))\n",
        "\n",
        "    noisy_labels = []\n",
        "    for i, sample in enumerate(train_data):\n",
        "        sample = sample.flatten()\n",
        "        p_all = np.matmul(sample,w)\n",
        "        p_all[train_labels[i]] = -1000000\n",
        "        p_all = q[i]* F.softmax(torch.tensor(p_all),dim=0).numpy()\n",
        "        p_all[train_labels[i]] = 1 - q[i]\n",
        "        noisy_labels.append(np.random.choice(np.arange(num_class),p=p_all/sum(p_all)))\n",
        "    over_all_noise_rate = 1 - float(torch.tensor(train_labels).eq(torch.tensor(noisy_labels)).sum())/50000\n",
        "    return noisy_labels, over_all_noise_rate"
      ],
      "metadata": {
        "id": "YmGiWz-deko9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset function"
      ],
      "metadata": {
        "id": "X7Ervjhcfk_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFAR10(data.Dataset):\n",
        "    \"\"\"`CIFAR10 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.\n",
        "\n",
        "    Args:\n",
        "        root (string): Root directory of dataset where directory\n",
        "            ``cifar-10-batches-py`` exists or will be saved to if download is set to True.\n",
        "        train (bool, optional): If True, creates dataset from training set, otherwise\n",
        "            creates from test set.\n",
        "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
        "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
        "        target_transform (callable, optional): A function/transform that takes in the\n",
        "            target and transforms it.\n",
        "        download (bool, optional): If true, downloads the dataset from the internet and\n",
        "            puts it in root directory. If dataset is already downloaded, it is not\n",
        "            downloaded again.\n",
        "\n",
        "    \"\"\"\n",
        "    base_folder = 'cifar-10-batches-py'\n",
        "    url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
        "    filename = \"cifar-10-python.tar.gz\"\n",
        "    tgz_md5 = 'c58f30108f718f92721af3b95e74349a'\n",
        "    train_list = [\n",
        "        ['data_batch_1', 'c99cafc152244af753f735de768cd75f'],\n",
        "        ['data_batch_2', 'd4bba439e000b95fd0a9bffe97cbabec'],\n",
        "        ['data_batch_3', '54ebc095f3ab1f0389bbae665268c751'],\n",
        "        ['data_batch_4', '634d18415352ddfa80567beed471001a'],\n",
        "        ['data_batch_5', '482c414d41f54cd18b22e5b47cb7c3cb'],\n",
        "    ]\n",
        "\n",
        "    test_list = [\n",
        "        ['test_batch', '40351d587109b95175f43aff81a1287e'],\n",
        "    ]\n",
        "\n",
        "    def __init__(self, root, train=True,\n",
        "                 transform=None, target_transform=None,\n",
        "                 download=False,\n",
        "                 noise_type=None, \n",
        "                 noise_path=None, \n",
        "                 is_human=True\n",
        "        ):\n",
        "        self.root = os.path.expanduser(root)\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.train = train  # training set or test set\n",
        "        self.dataset='cifar10'\n",
        "        self.noise_type=noise_type\n",
        "        self.nb_classes=10\n",
        "        self.noise_path = noise_path\n",
        "        idx_each_class_noisy = [[] for i in range(10)]\n",
        "        if download:\n",
        "           self.download()\n",
        "\n",
        "        # training data\n",
        "        if self.train:\n",
        "            self.train_data = []\n",
        "            self.train_labels = []\n",
        "            for fentry in self.train_list:\n",
        "                f = fentry[0]\n",
        "                file = os.path.join(self.root, self.base_folder, f)\n",
        "                fo = open(file, 'rb')\n",
        "                if sys.version_info[0] == 2:\n",
        "                    entry = pickle.load(fo)\n",
        "                else:\n",
        "                    entry = pickle.load(fo, encoding='latin1')\n",
        "                self.train_data.append(entry['data'])\n",
        "                if 'labels' in entry:\n",
        "                    self.train_labels += entry['labels']\n",
        "                else:\n",
        "                    self.train_labels += entry['fine_labels']\n",
        "                fo.close()\n",
        "\n",
        "            self.train_data = np.concatenate(self.train_data)\n",
        "            self.train_data = self.train_data.reshape((50000, 3, 32, 32))\n",
        "            self.train_data = self.train_data.transpose((0, 2, 3, 1))  # convert to HWC\n",
        "\n",
        "            #if noise_type is not None/clean:\n",
        "            if noise_type != 'clean_label':\n",
        "                # if is_human = True --> want human noise IDN\n",
        "                train_noisy_labels = self.load_label()\n",
        "                self.train_noisy_labels = train_noisy_labels.tolist()\n",
        "                print(f'noisy labels loaded from {self.noise_path}')\n",
        "\n",
        "                # if is_human = False --> want synthetic noise CCN with the same T\n",
        "                if not is_human:\n",
        "                    # create a CCN T from IDN human noisy labels, which has the same expected noise rate as IDN T\n",
        "                    T = np.zeros((self.nb_classes, self.nb_classes)) #10x10\n",
        "                    for i in range(len(self.train_noisy_labels)):\n",
        "                        T[self.train_labels[i]][self.train_noisy_labels[i]] += 1\n",
        "                    T = T/np.sum(T, axis=1) #row sum = 1\n",
        "                    print(f'Noise transition matrix is \\n{T}')\n",
        "\n",
        "                    # use this T to corrupt clean labels and obtain synthetic CCN noisy labels\n",
        "                    train_noisy_labels = multiclass_noisify(\n",
        "                                        y=np.array(self.train_labels), # clean labels\n",
        "                                        P=T, # the manually created CCN transition matrix\n",
        "                                        random_state=0\n",
        "                                    ) #np.random.randint(1,10086)\n",
        "                    self.train_noisy_labels = train_noisy_labels.tolist()\n",
        "\n",
        "                    # obtain another CCN T from above synthetic CCN noisy labels\n",
        "                    T = np.zeros((self.nb_classes, self.nb_classes))\n",
        "                    for i in range(len(self.train_noisy_labels)):\n",
        "                        T[self.train_labels[i]][self.train_noisy_labels[i]] += 1\n",
        "                    T = T/np.sum(T,axis=1)\n",
        "                    print(f'New synthetic noise transition matrix is \\n{T}')\n",
        "\n",
        "                for i in range(len(self.train_noisy_labels)):\n",
        "                    idx_each_class_noisy[self.train_noisy_labels[i]].append(i)\n",
        "                class_size_noisy = [len(idx_each_class_noisy[i]) for i in range(10)]\n",
        "                self.noise_prior = np.array(class_size_noisy)/sum(class_size_noisy)\n",
        "                print(f'The noisy data ratio in each class is {self.noise_prior}')\n",
        "                self.noise_or_not = np.transpose(self.train_noisy_labels)!=np.transpose(self.train_labels)\n",
        "                self.actual_noise_rate = np.sum(self.noise_or_not)/50000\n",
        "                print('over all noise rate is ', self.actual_noise_rate)\n",
        "\n",
        "        # test data\n",
        "        else:\n",
        "            f = self.test_list[0][0]\n",
        "            file = os.path.join(self.root, self.base_folder, f)\n",
        "            fo = open(file, 'rb')\n",
        "            if sys.version_info[0] == 2:\n",
        "                entry = pickle.load(fo)\n",
        "            else:\n",
        "                entry = pickle.load(fo, encoding='latin1')\n",
        "            self.test_data = entry['data']\n",
        "            if 'labels' in entry:\n",
        "                self.test_labels = entry['labels']\n",
        "            else:\n",
        "                self.test_labels = entry['fine_labels']\n",
        "            fo.close()\n",
        "            self.test_data = self.test_data.reshape((10000, 3, 32, 32))\n",
        "            self.test_data = self.test_data.transpose((0, 2, 3, 1))  # convert to HWC\n",
        "\n",
        "    # load human annotated noisy label (CIFAR-10)\n",
        "    def load_label(self):\n",
        "        noise_label = torch.load(self.noise_path)\n",
        "        if isinstance(noise_label, dict):\n",
        "            if \"clean_label\" in noise_label.keys():\n",
        "                clean_label = torch.tensor(noise_label['clean_label'])\n",
        "                assert torch.sum(torch.tensor(self.train_labels) - clean_label) == 0  \n",
        "                print(f'Loaded {self.noise_type} from {self.noise_path}.')\n",
        "                print(f'The overall noise rate is {1-np.mean(clean_label.numpy() == noise_label[self.noise_type])}')\n",
        "            return noise_label[self.noise_type].reshape(-1)  \n",
        "        else:\n",
        "            raise Exception('Input Error')\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "\n",
        "        Returns:\n",
        "            tuple: (image, target) where target is index of the target class.\n",
        "        \"\"\"\n",
        "        if self.train:\n",
        "            if self.noise_type != 'clean_label':\n",
        "                img, target = self.train_data[index], self.train_noisy_labels[index]\n",
        "            else:\n",
        "                img, target = self.train_data[index], self.train_labels[index]\n",
        "        else:\n",
        "            img, target = self.test_data[index], self.test_labels[index]\n",
        "\n",
        "        # doing this so that it is consistent with all other datasets\n",
        "        # to return a PIL Image\n",
        "        img = Image.fromarray(img)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return img, target, index\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.train:\n",
        "            return len(self.train_data)\n",
        "        else:\n",
        "            return len(self.test_data)\n",
        "\n",
        "    def _check_integrity(self):\n",
        "        root = self.root\n",
        "        for fentry in (self.train_list + self.test_list):\n",
        "            filename, md5 = fentry[0], fentry[1]\n",
        "            fpath = os.path.join(root, self.base_folder, filename)\n",
        "            if not check_integrity(fpath, md5):\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def download(self):\n",
        "        import tarfile\n",
        "\n",
        "        if self._check_integrity():\n",
        "            print('Files already downloaded and verified')\n",
        "            return\n",
        "\n",
        "        root = self.root\n",
        "        download_url(self.url, root, self.filename, self.tgz_md5)\n",
        "\n",
        "        # extract file\n",
        "        cwd = os.getcwd()\n",
        "        tar = tarfile.open(os.path.join(root, self.filename), \"r:gz\")\n",
        "        os.chdir(root)\n",
        "        tar.extractall()\n",
        "        tar.close()\n",
        "        os.chdir(cwd)\n",
        "\n",
        "    def __repr__(self):\n",
        "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
        "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
        "        tmp = 'train' if self.train is True else 'test'\n",
        "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
        "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
        "        tmp = '    Transforms (if any): '\n",
        "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
        "        tmp = '    Target Transforms (if any): '\n",
        "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
        "        return fmt_str\n"
      ],
      "metadata": {
        "id": "kRIEZOoKfJ9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "train_cifar10_transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4), \n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "test_cifar10_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])"
      ],
      "metadata": {
        "id": "W3xOnj_TocNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def input_dataset(noise_type, noise_path, is_human):\n",
        "      train_dataset = CIFAR10(root='~/data/',\n",
        "                              download=True,  \n",
        "                              train=True, \n",
        "                              transform = train_cifar10_transform,\n",
        "                              noise_type = noise_type, \n",
        "                              noise_path = noise_path, \n",
        "                              is_human=is_human # decide if train_noisy_labels are human noisy labels or synthetic CCN noisy labels\n",
        "                        )\n",
        "      test_dataset = CIFAR10(root='~/data/',\n",
        "                              download=False,  \n",
        "                              train=False, \n",
        "                              transform = test_cifar10_transform,\n",
        "                              noise_type = noise_type\n",
        "                        )\n",
        "      num_classes = 10\n",
        "      num_training_samples = 50000\n",
        "   \n",
        "      return train_dataset, test_dataset, num_classes, num_training_samples"
      ],
      "metadata": {
        "id": "AnyOWwy2pMdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Functions (ResNet)"
      ],
      "metadata": {
        "id": "ThojYoaTrXvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''ResNet in PyTorch.\n",
        "\n",
        "For Pre-activation ResNet, see 'preact_resnet.py'.\n",
        "\n",
        "Reference:\n",
        "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
        "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class PreActBlock(nn.Module):\n",
        "    '''Pre-activation version of the BasicBlock.'''\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(PreActBlock, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(x))\n",
        "        shortcut = self.shortcut(out)\n",
        "        out = self.conv1(out)\n",
        "        out = self.conv2(F.relu(self.bn2(out)))\n",
        "        out += shortcut\n",
        "        return out\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class PreActBottleneck(nn.Module):\n",
        "    '''Pre-activation version of the original Bottleneck module.'''\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(PreActBottleneck, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(x))\n",
        "        shortcut = self.shortcut(out)\n",
        "        out = self.conv1(out)\n",
        "        out = self.conv2(F.relu(self.bn2(out)))\n",
        "        out = self.conv3(F.relu(self.bn3(out)))\n",
        "        out += shortcut\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def PreResNet18(num_classes):\n",
        "    return ResNet(PreActBlock, [2,2,2,2],num_classes=num_classes)\n",
        "\n",
        "def ResNet18(num_classes):\n",
        "    return ResNet(BasicBlock, [2,2,2,2],num_classes=num_classes)\n",
        "\n",
        "def ResNet34(num_classes):\n",
        "    return ResNet(BasicBlock, [3,4,6,3],num_classes=num_classes)\n",
        "\n",
        "def ResNet50(num_classes):\n",
        "    return ResNet(Bottleneck, [3,4,6,3],num_classes=num_classes)\n",
        "\n",
        "def ResNet101(num_classes):\n",
        "    return ResNet(Bottleneck, [3,4,23,3],num_classes=num_classes)\n",
        "\n",
        "def ResNet152(num_classes):\n",
        "    return ResNet(Bottleneck, [3,8,36,3],num_classes=num_classes)\n",
        "\n",
        "\n",
        "def test():\n",
        "    net = ResNet18()\n",
        "    y = net(torch.randn(1,3,32,32))\n",
        "    print(y.size())\n"
      ],
      "metadata": {
        "id": "63eDl7sEraCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Testing Functions"
      ],
      "metadata": {
        "id": "28z-h67Kp2u6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learning Rate Scheduler"
      ],
      "metadata": {
        "id": "vAlxC3fzIqUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# adjust learning rate based on the current epoch, according to the alpha plan\n",
        "def adjust_learning_rate(optimizer, epoch, alpha_plan):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr']=alpha_plan[epoch]"
      ],
      "metadata": {
        "id": "Y4iTEgpXpu8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate Accuracy"
      ],
      "metadata": {
        "id": "8B9hyD97IwBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(logit, target, topk=(1,)):\n",
        "    \"\"\"computes the precision@k for the specified values of k\"\"\"\n",
        "    output = F.softmax(logit, dim=1)\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res"
      ],
      "metadata": {
        "id": "pPTspY_TIkWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Model\n",
        "def train(epoch, train_loader, model, optimizer):\n",
        "    train_total=0\n",
        "    train_correct=0\n",
        "\n",
        "    for i, (images, labels, indexes) in enumerate(train_loader):\n",
        "        ind=indexes.cpu().numpy().transpose()\n",
        "        batch_size = len(ind)\n",
        "       \n",
        "        images = Variable(images).cuda()\n",
        "        labels = Variable(labels).cuda()\n",
        "       \n",
        "        # Forward + Backward + Optimize\n",
        "        logits = model(images)\n",
        "\n",
        "        prec, _ = accuracy(logits, labels, topk=(1, 5))\n",
        "        # prec = 0.0\n",
        "        train_total+=1\n",
        "        train_correct+=prec\n",
        "        loss = F.cross_entropy(logits, labels, reduce = True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if (i+1) % args.print_freq == 0:\n",
        "            print ('Epoch [%d/%d], Iter [%d/%d] Training Accuracy: %.4F, Loss: %.4f'\n",
        "                  %(epoch+1, args.n_epoch, i+1, len(train_dataset)//batch_size, prec, loss.data))\n",
        "\n",
        "    train_acc=float(train_correct)/float(train_total)\n",
        "    return train_acc"
      ],
      "metadata": {
        "id": "l29hH6wDIWiF"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the Model\n",
        "def evaluate(test_loader, model):\n",
        "    model.eval()    # Change model to 'eval' mode.\n",
        "    # print('previous_best', best_acc_)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels, _ in test_loader:\n",
        "        images = Variable(images).cuda()\n",
        "        logits = model(images)\n",
        "        outputs = F.softmax(logits, dim=1)\n",
        "        _, pred = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (pred.cpu() == labels).sum()\n",
        "    acc = 100*float(correct)/float(total)\n",
        "\n",
        "    return acc"
      ],
      "metadata": {
        "id": "GxZ1LY75sEM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss"
      ],
      "metadata": {
        "id": "jlTuOf3orSSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "\n",
        "# Loss functions\n",
        "def loss_cross_entropy(epoch, y, t,class_list, ind, noise_or_not,loss_all,loss_div_all):\n",
        "    ## Record loss and loss_div for further analysis\n",
        "    loss = F.cross_entropy(y, t, reduce = False)\n",
        "    loss_numpy = loss.data.cpu().numpy()\n",
        "    num_batch = len(loss_numpy)\n",
        "    loss_all[ind,epoch] = loss_numpy\n",
        "\n",
        "    return torch.sum(loss)/num_batch"
      ],
      "metadata": {
        "id": "fthGQbwzrRkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Code"
      ],
      "metadata": {
        "id": "84NmLQJRp6V6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "\n",
        "# specify noise paramaters\n",
        "noise_type_map = {\n",
        "              'clean':'clean_label', \n",
        "              'worst': 'worse_label', \n",
        "              'aggre': 'aggre_label', \n",
        "              'rand1': 'random_label1', \n",
        "              'rand2': 'random_label2', \n",
        "              'rand3': 'random_label3'\n",
        "            }\n",
        "noise_type = noise_type_map['clean']\n",
        "noise_path = './data/CIFAR-10_human.pt'\n",
        "is_human = True\n",
        "\n",
        "# obtain corresponding datasets and create iterators\n",
        "train_dataset, test_dataset, num_classes, num_training_samples = input_dataset(\n",
        "    noise_type, \n",
        "    noise_path, \n",
        "    is_human\n",
        ")\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
        "                                   batch_size = 128,\n",
        "                                   num_workers=4,\n",
        "                                   shuffle=True)\n",
        "\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                  batch_size = 64,\n",
        "                                  num_workers=4,\n",
        "                                  shuffle=False)\n",
        "\n",
        "noise_prior = train_dataset.noise_prior\n",
        "noise_or_not = train_dataset.noise_or_not\n",
        "\n",
        "# build model\n",
        "model = ResNet34(num_classes)\n",
        "\n",
        "# learning rate, learning rate scheduler and optimizer\n",
        "learning_rate = 0.1\n",
        "alpha_plan = [0.1] * 60 + [0.01] * 40\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=0.0005, momentum=0.9)\n",
        "\n",
        "model.cuda()\n",
        "\n",
        "\n",
        "epoch=0\n",
        "train_acc = 0\n",
        "n_epoch = 100\n",
        "# training\n",
        "noise_prior_cur = noise_prior\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "# train models\n",
        "    print(f'epoch {epoch}')\n",
        "    # adjust learning rate at the start of every epoch\n",
        "    adjust_learning_rate(optimizer, epoch, alpha_plan)\n",
        "    model.train()\n",
        "    train_acc = train(epoch, train_loader, model, optimizer)\n",
        "    # evaluate models\n",
        "    test_acc = evaluate(test_loader, model)\n",
        "    # save results\n",
        "    print('train acc on train images is ', train_acc)\n",
        "    print('test acc on test images is ', test_acc)\n"
      ],
      "metadata": {
        "id": "Qs7Mdp4kp7l6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mbW2G-nBLlJ7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}